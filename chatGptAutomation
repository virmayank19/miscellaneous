import org.apache.iceberg.Files;
import org.apache.iceberg.Schema;
import org.apache.iceberg.hadoop.HadoopOutputFile;
import org.apache.iceberg.io.OutputFile;
import org.apache.iceberg.parquet.Parquet;
import org.apache.iceberg.data.GenericRecord;
import org.apache.iceberg.data.Record;
import org.apache.iceberg.types.Types;
import java.io.File;
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;

public class IcebergParquetWriter {

    public static void main(String[] args) throws IOException {
        // Define the schema
        Schema schema = new Schema(
                Types.NestedField.required(1, "id", Types.IntegerType.get()),
                Types.NestedField.optional(2, "name", Types.StringType.get())
        );

        // Create some sample records
        List<Record> records = new ArrayList<>();
        records.add(GenericRecord.create(schema).setField("id", 1).setField("name", "Alice"));
        records.add(GenericRecord.create(schema).setField("id", 2).setField("name", "Bob"));

        // Define the output file path
        File outputFile = new File("users.parquet");
        OutputFile out = HadoopOutputFile.fromPath(outputFile.toPath());

        // Write the records to the Parquet file
       try (org.apache.iceberg.io.FileWriter<Record> writer = Parquet.write(out)
                .schema(schema)
                .named("test-table")
                .build()) {
            for (Record record : records) {
                writer.write(record);
            }
        }
        System.out.println("Data written to " + outputFile.getAbsolutePath());
    }
}
