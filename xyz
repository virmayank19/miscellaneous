import langchain
from langchain.llms import Llama2
from langchain.vectorstores import FAISS

# Load the Llama2 model
llm = Llama2(checkpoint="llama2-chat-13B")  # Substitute with your desired Llama2 checkpoint

# Create a FAISS vectorstore for retrieval
vectorstore = FAISS()

# Load the text document, chunk it, and add to the vectorstore
with open("your_text_document.txt", "r") as f:
    text = f.read()
    chunks = langchain.utils.chunk_text(text, chunk_size=512)  # Adjust chunk size
    vectorstore.add_documents(chunks)

# Define a function to process prompts and generate responses
def generate_response(user_prompt, system_prompt):
    query = f"{user_prompt}\n{system_prompt} {llm.generate_end_of_text()}"
    response = llm(query)
    return response

# Get user prompt and system prompt
user_prompt = input("Enter your prompt: ")
system_prompt = input("Enter the system prompt: ")

# Retrieve relevant chunks based on user prompt
relevant_chunks = vectorstore.query(user_prompt, k=5)  # Adjust number of retrieved chunks

# Combine relevant chunks with system prompt and generate response
combined_prompt = "\n".join(relevant_chunks) + "\n" + system_prompt
response = generate_response(user_prompt, combined_prompt)

print(response)







import langchain
from langchain.vectorstores import FAISS
from sentence_transformers import SentenceTransformer  # Import Sentence Transformers

# Load the SentenceTransformer model
model = SentenceTransformer('all-mpnet-base-v2')  # Choose a suitable model

# Define the embeddings function using Sentence Transformers
def embeddings_function(text):
    return model.encode(text)

# Create the FAISS vectorstore with the embeddings function
vectorstore = FAISS(embeddings_function=embeddings_function)

# Load the text document, chunk it, and add to the vectorstore
with open("your_text_document.txt", "r") as f:
    text = f.read()
    chunks = langchain.utils.chunk_text(text, chunk_size=512)  # Adjust chunk size as needed
    vectorstore.add_documents(chunks)

# Get user prompt
user_prompt = input("Enter your prompt: ")

# Retrieve relevant chunks based on user prompt
relevant_chunks = vectorstore.query(user_prompt, k=5)  # Adjust number of retrieved chunks

print(relevant_chunks)

