import numpy as np
from sentence_transformers import SentenceTransformer
from annoy import AnnoyIndex
from data_definitions import data

# Create text embeddings for each dataset
models = []
embeddings = []
for i in range(1, 11):
    model = SentenceTransformer(f'all-mpnet-base-v2-{i}')  # Choose a suitable model for each dataset
    models.append(model)
    dataset = f'data{i}'
    embeddings.append(model.encode(list(data[dataset].values())))

# Build vector indexes for each dataset
indexes = []
for i, emb in enumerate(embeddings):
    f = 768  # Dimensionality of the embeddings from the 'all-mpnet-base-v2' model
    t = AnnoyIndex(f, 'angular')  # Use angular distance for text similarity
    for j, embedding in enumerate(emb):
        t.add_item(j, embedding)
    t.build(10)  # Adjust the number of trees for performance
    indexes.append(t)

def vector_search(query, top_k=5):
    best_match_dataset = None
    best_match_distance = float('inf')

    for i, index in enumerate(indexes):
        query_embedding = models[i].encode([query])[0]
        nns = index.get_nns_by_vector(query_embedding, top_k, include_distances=True)

        avg_distance = np.mean(nns[1])

        if avg_distance < best_match_distance:
            best_match_distance = avg_distance
            best_match_dataset = f'data{i + 1}'

    results = []
    for idx, distance in zip(nns[0], nns[1]):
        field_name = list(data[best_match_dataset].keys())[idx]
        results.append((field_name, distance))

    return results, best_match_dataset

# Example usage
query = "location"
results, best_match_dataset = vector_search(query, top_k=5)
print(f"Top 5 matching results for '{query}' in the most relevant dataset '{best_match_dataset}':")
for result in results:
    print(result[0], " - Distance:", result[1])
